{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch - Tensors!\n",
    "\n",
    "\n",
    "This notebook focus on the use of tensors in Pytorch. This material is the same available by Matthew Mayo in [KDnuggets](https://www.kdnuggets.com/2018/05/pytorch-tensor-basics.html).\n",
    "\n",
    "\n",
    "**@notebook_author: [Juarez Monteiro](https://jrzmnt.github.io).**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This is an introduction to PyTorch's Tensor class, which is reasonably analogous to Numpy's ndarray, and which forms the basis for building neural networks in PyTorch.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[**PyTorch**](https://pytorch.org/) has made an impressive dent on the machine learning scene since Facebook open-sourced it in early 2017. \n",
    "It may not have the widespread adoption that TensorFlow has -- which was initially released well over a year prior, enjoys the backing of Google, and had the luxury of establishing itself as the gold standard as a new wave of neural networking tools was being ushered in -- but the attention that PyTorch receives in the research community especially is quite real. \n",
    "Much of this attention comes both from its relationship to Torch proper, and its dynamic computation graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![img](https://www.kdnuggets.com/wp-content/uploads/tensor-examples.jpg \"Example of tensors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As excited as I have recently been by turning my own attention to PyTorch, this is **not** really a **PyTorch tutorial**; it's more of an **introduction to PyTorch's Tensor class**, which is reasonably analogous to Numpy's ndarray.m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensor (Very) Basics\n",
    "\n",
    "So let's take a look at some of PyTorch's tensor basics, starting with creating a tensor (using the **Tensor class**):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create a Torch tensor\n",
    "t = torch.Tensor([[1, 2, 3], [4, 5, 6]])\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### You can transpose a tensor in one of 2 ways:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  4.],\n",
       "        [ 2.,  5.],\n",
       "        [ 3.,  6.]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose\n",
    "t.t()\n",
    "\n",
    "# Transpose (via permute)\n",
    "t.permute(-1,0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Note that neither result in a change to the original."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.,  3.],\n",
       "        [ 4.,  5.,  6.]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Reshape a tensor with view:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.,  2.],\n",
       "        [ 3.,  4.],\n",
       "        [ 5.,  6.]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape via view\n",
    "#t = t.view(3,2)\n",
    "t.view(3,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 1.],\n",
       "        [ 2.],\n",
       "        [ 3.],\n",
       "        [ 4.],\n",
       "        [ 5.],\n",
       "        [ 6.]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just another example\n",
    "t.view(6,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*It should be obvious that mathematical conventions which are followed by **Numpy** carry over to **PyTorch Tensors** (specifically I'm referring to row and column notation).*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a tensor and fill it with zeros (you can accomplish something similar with ones()):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.],\n",
       "        [ 0.,  0.,  0.]])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.zeros(3,3)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a tensor with randoms pulled from the normal distribution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.5573,  1.4828,  0.7756],\n",
       "        [ 0.2646, -0.6887,  0.7479],\n",
       "        [ 1.7068, -0.4491, -0.0172]])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = torch.randn(3,3)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Shape, dimensions, and datatype of a tensor object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Tensor shape:', torch.Size([3, 3]))\n",
      "('Number of dimensions:', 2)\n",
      "('Tensor type:', 'torch.FloatTensor')\n"
     ]
    }
   ],
   "source": [
    "# Some tensor info\n",
    "print('Tensor shape:', t.shape)   # t.size() gives the same\n",
    "print('Number of dimensions:', t.dim())\n",
    "print('Tensor type:', t.type())   # there are other types"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It should also be obvious that, beyond mathematical concepts, a number of programmatic and instantiation similarities between ndarray and Tensor implementations exist.\n",
    "\n",
    "You can **slice PyTorch tensors** the same way you **slice ndarrays**, which should be familiar to anyone who uses other Python structures:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.],\n",
      "        [ 7.,  8.,  9.]])\n",
      "tensor([ 3.,  6.,  9.])\n",
      "tensor([[ 1.,  2.,  3.],\n",
      "        [ 4.,  5.,  6.]])\n",
      "tensor([[ 9.]])\n"
     ]
    }
   ],
   "source": [
    "# Slicing\n",
    "t = torch.Tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]])\n",
    "print t\n",
    "\n",
    "# Every row, only the last column\n",
    "print(t[:, -1])\n",
    "\n",
    "# First 2 rows, all columns\n",
    "print(t[:2, :])\n",
    "\n",
    "# Lower right most corner\n",
    "print(t[-1:, -1:])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PyTorch Tensor To and From Numpy ndarray\n",
    "\n",
    "You can easily create a tensors from an ndarray and vice versa. These operations are fast, since the data of both structures will share the same memory space, and so no copying is involved. This is obviously an efficient approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-1.19342516 -2.20770196  0.05667432 -0.07387365 -0.25733323]\n",
      " [-2.4731455  -0.21433011  1.61143989 -1.6815513  -1.85775728]\n",
      " [-0.27608511  1.20422478 -0.15524044 -2.03573287  0.21608626]]\n",
      "tensor([[-1.1934, -2.2077,  0.0567, -0.0739, -0.2573],\n",
      "        [-2.4731, -0.2143,  1.6114, -1.6816, -1.8578],\n",
      "        [-0.2761,  1.2042, -0.1552, -2.0357,  0.2161]], dtype=torch.float64)\n",
      "<type 'numpy.ndarray'>\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "# ndarray to tensor\n",
    "a = np.random.randn(3, 5)\n",
    "t = torch.from_numpy(a)\n",
    "print(a)\n",
    "print(t)\n",
    "print(type(a))\n",
    "print(type(t))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.7276, -0.1433,  0.9729,  0.0480, -1.1742],\n",
      "        [ 1.3700,  0.2172,  0.2750,  0.4016, -1.0471],\n",
      "        [ 0.8151,  0.1270, -1.8187, -0.6939,  0.8868]])\n",
      "[[-1.7276318  -0.1433103   0.97291845  0.04797237 -1.1742376 ]\n",
      " [ 1.3699548   0.21723264  0.27496466  0.40157077 -1.0470865 ]\n",
      " [ 0.81514406  0.12698859 -1.8186685  -0.6939064   0.886809  ]]\n",
      "<class 'torch.Tensor'>\n",
      "<type 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "# tensor to ndarray\n",
    "t = torch.randn(3, 5)\n",
    "a = t.numpy()\n",
    "print(t)\n",
    "print(a)\n",
    "print(type(t))\n",
    "print(type(a))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Tensor Operations\n",
    "\n",
    "Here are a **few tensor operations**, which you can compare with Numpy implementations for fun. First up is the **cross product**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.7481, -1.1264, -1.0862],\n",
      "        [ 0.3520, -0.9261, -1.0435],\n",
      "        [-0.0773, -1.0966, -0.8887],\n",
      "        [-0.5405, -1.2386,  0.9814]])\n",
      "tensor([[ 2.2663,  0.3275,  1.5255],\n",
      "        [-0.1170,  0.7240, -2.1284],\n",
      "        [-2.1810, -0.0520, -0.9417],\n",
      "        [ 0.0301, -1.1192, -0.1893]])\n",
      "tensor([[-1.3626, -3.6029,  2.7977],\n",
      "        [ 2.7266,  0.8712,  0.1465],\n",
      "        [ 0.9865,  1.8655, -2.3877],\n",
      "        [ 1.3328, -0.0728,  0.6422]])\n"
     ]
    }
   ],
   "source": [
    "# Compute cross product\n",
    "t1 = torch.randn(4,3)\n",
    "t2 = torch.randn(4,3)\n",
    "print t1\n",
    "print t2\n",
    "print t1.cross(t2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
